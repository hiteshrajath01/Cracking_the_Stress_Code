# -*- coding: utf-8 -*-
"""Execution_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nJI_nOCkcmhe0YJLle_fR5ihF8DCW0VN

####Import Libraries
"""

import shutil
import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from datetime import datetime, time
from sklearn.utils import resample
from matplotlib.dates import DateFormatter, AutoDateLocator
from tensorflow import keras
from keras import layers, models
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import RandomizedSearchCV
from keras.layers import LSTM, Bidirectional
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, confusion_matrix

"""####Set Path in Google Drive"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./DS/') # Make sure to update with your student_id and student_id is an integer
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

"""#### Read csv from Google drive and set into dictionary"""

# Name of the zip file to extract
BaseZipFolderPath=GOOGLE_DRIVE_PATH

# Set Input directory
input_folderpath=BaseZipFolderPath+'Input_data/'

# Create an empty dictionary with keys ranging from 02 to 35
id_dfs = {f"{i:02d}": None for i in range(2, 36)}

# Loop through the dictionary keys and load the corresponding CSV file
for key in id_dfs.keys():
    filename = os.path.join(input_folderpath, f"{key}.csv")
    id_dfs[key] = pd.read_csv(filename)

# Print the dictionary
print(id_dfs)

"""##Common functions

####Drop id column and set datetime column as index
"""

#Function to drop id column and convert datetime column to index
def drop_id_datetime(df):
    # Set datetime column as index
    df = df.set_index('datetime')

    # Drop the id column
    df = df.drop('id', axis=1)

    return df

#Drop id and set datetime column as index
for i in id_dfs.keys(): 
  id_dfs[i] = drop_id_datetime(id_dfs[i])

"""####Create sliding window"""

# Create sliding window
def sliding_window(data, window_size, step_size):
    # Create lists for windows and labels
    windows = []
    labels = []

    # Segment time series into windows
    for i in range(0, len(data)-window_size, step_size):
        window = data.iloc[i:i+window_size]
        label = window['label'].iloc[-1]
        windows.append(window.drop(['label'], axis=1).values)
        labels.append(label)

    # Prepare data for model
    X = np.array(windows)
    y = np.array(labels)

    X = X.reshape((X.shape[0], window_size, X.shape[2]))
    return X, y

"""#### Split data into train,validation,test sets on ratio 60:20:20 """

#Function to create train, validation and test set while preserving the temporal aspect of the data
def create_train_val_test(X,y, train_ratio=0.6, val_ratio=0.2):
    n_train = int(train_ratio * len(X))
    n_val = int(val_ratio * len(X))
    n_test = len(X) - n_train - n_val
    
    # Split into train
    X_train = X[:n_train]
    y_train = y[:n_train]

    # Split into validation
    X_val = X[n_train:n_train+n_val]
    y_val = y[n_train:n_train+n_val]

    # Split into test
    X_test = X[n_train+n_val:]
    y_test = y[n_train+n_val:]
    
    # Display the count of train,validation and test set
    print('X Train count: ' + str(len(X_train)))
    print('y Train count: ' + str(len(y_train)))
    print('X Val count: ' + str(len(X_val)))
    print('y Val count: ' + str(len(y_val)))
    print('X Test count: ' + str(len(X_test)))
    print('y Test count: ' + str(len(y_test)))
    
    return X_train, y_train, X_val, y_val, X_test, y_test

"""#### Flatten the data to 2 dimensions"""

# Flatten the data to 2 dimensions
def flatten_data(X_train,X_val,X_test):
  X_train_flat = X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2]))
  X_val_flat = X_val.reshape((X_val.shape[0]*X_val.shape[1], X_val.shape[2]))
  X_test_flat = X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))
  return X_train_flat,X_val_flat,X_test_flat

"""####Normalize the data"""

#Function to Normalize the data using Standard scalar
def scale_data(X_Train,X_Val,X_Test):
  scaler = StandardScaler()
  X_train_normalized = scaler.fit_transform(X_Train)
  X_val_normalized = scaler.transform(X_Val)
  X_test_normalized = scaler.transform(X_Test)
  return X_train_normalized,X_val_normalized,X_test_normalized

"""#### Reshape data back to 3D array"""

# Reshape the normalized data back to 3 dimensions
def reshape_3d(X_train_norm,X_train,X_val_norm,X_val,X_test_norm,X_test):
  X_train_norm = X_train_norm.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))
  X_val_norm = X_val_norm.reshape((X_val.shape[0], X_val.shape[1], X_val.shape[2]))
  X_test_norm = X_test_norm.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))
  return X_train_norm,X_val_norm,X_test_norm

"""####Add label back to normalized features"""

# Add label back to the normalized features
def combine_feature_label(X_train_normalized,y_train,X_val_normalized,y_val,X_test_normalized,y_test):
  train_data_norm = np.column_stack((X_train_normalized, y_train))
  val_data_norm = np.column_stack((X_val_normalized, y_val))
  test_data_norm = np.column_stack((X_test_normalized, y_test))
  return train_data_norm,val_data_norm,test_data_norm

"""####Evaluate model"""

def EvaluateModel(y_test,y_test_pred_model):
  # Convert probabilities to binary predictions for classification
  y_test_pred_binary = np.round(y_test_pred_model)

  # Calculate evaluation metrics
  accuracy = accuracy_score(y_test, y_test_pred_binary)
  f1 = f1_score(y_test, y_test_pred_binary)
  precision = precision_score(y_test, y_test_pred_binary)
  recall = recall_score(y_test, y_test_pred_binary)
  fp, tp, thresholds = roc_curve(y_test, y_test_pred_model)
  cm = confusion_matrix(y_test, y_test_pred_binary)

  # Plot ROC curve
  plt.plot(fp, tp)
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('ROC Curve')
  plt.show()

  # Print evaluation metrics and confusion matrix
  print(f'Accuracy: {accuracy:.4f}')
  print(f'F1 score: {f1:.4f}')
  print(f'Precision: {precision:.4f}')
  print(f'Recall: {recall:.4f}')
  print(f'Confusion Matrix:\n{cm}')

"""####Train model1"""

def TrainModel1(train_data,y_train):
  # define parameter ranges for random search
  param_dist = {
      'n_estimators': [50, 100, 200],
      'max_features': ['auto', 'sqrt'],
      'max_depth': [10, 20, 30, None],
      'min_samples_split': [2, 5, 10],
      'min_samples_leaf': [1, 2, 4],
      'bootstrap': [True, False]
  }

  # define random search with time series cross-validation
  rfc = RandomForestClassifier()
  tscv = TimeSeriesSplit(n_splits=5)
  random_search = RandomizedSearchCV(estimator=rfc, 
                                    param_distributions=param_dist, 
                                    n_iter=20, 
                                    scoring='accuracy', 
                                    cv=tscv, 
                                    verbose=1, 
                                    n_jobs=-1)

  # fit the random search to the training data
  random_search.fit(train_data, y_train)

  # print the best hyperparameters and corresponding validation score
  print("Best parameters found: ", random_search.best_params_)

  # get the best hyperparameters
  best_params = random_search.best_params_

  # train a new random forest model using the best hyperparameters
  rfc_best = RandomForestClassifier(**best_params)
  rfc_best.fit(train_data, y_train)
  return rfc_best

"""####Test model1"""

# Function to test the model on validation and test set
def TestModel1(model1,val_data_norm,test_data_norm):
  y_val_pred_model1 = model1.predict(val_data_norm)
  y_test_pred_model1 = model1.predict(test_data_norm)
  return y_val_pred_model1,y_test_pred_model1

"""####Train model2"""

def TrainModel2(X_train_win,y_train_win,X_val_win,y_val_win,window_size):
  # reset the TensorFlow graph
  tf.keras.backend.clear_session()

  # Define the model
  num_features = X_train.shape[2]
  model = Sequential()
  model.add(LSTM(16, input_shape=(window_size, num_features)))
  model.add(Dropout(0.3))
  model.add(Dense(1, activation='sigmoid'))
  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

  # train the model
  history = model.fit(X_train_win, y_train_win, validation_data=(X_val_win, y_val_win), epochs=20, batch_size=64)

  return history,model

"""####Test model2"""

def TestModel2(model,X_test_win):
  y_pred_probability = model.predict(X_test_win)
  y_test_pred_model2 = (y_pred_probability > 0.5).astype(int)
  return y_test_pred_model2

"""####Train model3"""

def TrainModel3(X_train, y_train, X_val, y_val,window_size):
  # Reset the TensorFlow graph
  tf.keras.backend.clear_session()

  num_features = X_train.shape[2]
  model = Sequential()
  model.add(Bidirectional(LSTM(16), input_shape=(window_size, num_features)))
  model.add(Dropout(0.3))
  model.add(Dense(1, activation='sigmoid'))
  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

  # train the model
  history = model.fit(X_train, y_train, validation_data=(y_train, y_val), epochs=20, batch_size=64)  

  return history, model

"""Test model3"""

def TestModel3(model,X_test_win):
  y_pred_probability = model.predict(X_test_win)
  y_test_pred_model2 = (y_pred_probability > 0.5).astype(int)
  return y_test_pred_model2

"""####Plot loss"""

def plot_loss(history):
    # Plot model loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])

    # Add labels and legend
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(['Train', 'Val'], loc='upper right')

    # Show the plot
    plt.show()

"""##Execute the functions"""

#Sliding window concept is applied on neural network models : Model2:LSTM and Model3: BiLSTM,   
#Sliding window concept not applied for model1: randomforest


#Execute function
windows=['without_sliding_window','with_sliding_window']
for i in id_dfs.keys():
  for window in windows:
    if window=='without_sliding_window':
      print('----------------------------------------------------------------------------***********Start**********----------------------------------------------------------------------------------------------------------')
      print('Results for participant:'+i)
      
      # Separate X and y
      X = id_dfs[i].drop('label', axis=1)  # drop the 'label' column to create the feature matrix
      y = id_dfs[i]['label'] 

      #Split the data
      X_train, y_train, X_val, y_val, X_test, y_test = create_train_val_test(X,y)

      #Normalize the data
      X_train_normalized,X_val_normalized,X_test_normalized=scale_data(X_train,X_val,X_test)

      print('**********************************************************************************Model1 : Random forest***************************************************************************************')
      #Train model 1
      model1=TrainModel1(X_train_normalized,y_train)

      #Test model 1
      y_val_pred_model1,y_test_pred_model1=TestModel1(model1,X_val_normalized,X_test_normalized)

      #Evaluate model 1
      EvaluateModel(y_test,y_test_pred_model1)
      print('**********************************************************************************Model2 : LSTM****************************************************************************************')
    else:
      #Set Window size and step size
      window=60
      step_size=10

      #Sliding window
      X, y=sliding_window(id_dfs[i], window, step_size)

      #Split the data
      X_train, y_train, X_val, y_val, X_test, y_test = create_train_val_test(X,y)

      #Flatten the array to 2D array from 3D array
      X_train_flat,X_val_flat,X_test_flat=flatten_data(X_train,X_val,X_test)

      #Scale the data
      X_train_normalized,X_val_normalized,X_test_normalized=scale_data(X_train_flat,X_val_flat,X_test_flat)

      #Reshape the data from 2D array back to 3Darray
      X_train_reshape,X_val_reshape,X_test_reshape=reshape_3d(X_train_normalized,X_train,X_val_normalized,X_val,X_test_normalized,X_test)

      #Train model 2
      history_2,model2=TrainModel2(X_train_reshape,y_train,X_val_reshape,y_val,window)

      #Test model 2
      y_test_pred_model2=TestModel2(model2,X_test_reshape)

      #Evaluate model 2
      EvaluateModel(y_test,y_test_pred_model2)
      plot_loss(history_2)

      print('*********************************************************************************Model3 : BiLSTM*******************************************************************************************************')
      #Train model 3
      history_3,model3=TrainModel2(X_train_reshape,y_train,X_val_reshape,y_val,window)

      #Test model 3
      y_test_pred_model3=TestModel2(model3,X_test_reshape)

      #Evaluate model 3
      EvaluateModel(y_test,y_test_pred_model3)
      plot_loss(history_3)
      print('-----------------------------------------------------------------------------************End****************-----------------------------------------------------------------------------------------------')